The pipeline in autoeval-coordinator produces detailed logs both for the status of the pipeline itself and for the jobs dispatched to Nomad by the pipeline. For a single pipeline it is enough to inspect these logs to assess pipeline success or diagnose why a pipeline is failing. However, things become more challenging when many pipelines are run as a batch. Sometimes individual pipelines can fail for non-deterministic reasons (such as a Nomad node failure) while most other pipelines succeed. To make it easier to assess the outcome of running a batch of pipelines this repository contains code to scrape the logs from a batches' pipelines to produce summary reports of successful and failed pipelines.  

# Interpreting CloudWatch reports

The outcome of a batch run with `submit_stac_batch.py` on the NGWPC Test account can be assessed by running the `cloudwatch_reports.py` script in the `tools` directory. This script is a series of CloudWatch queries that scrapes the logs produced by all the pipelines in a batch. Instructions for generating the reports can be found in `docs/batch-run-guide-AWS-Test.md`.

Once the reports have been generated there will be a report directory containing a number of .txt and .json files. The text files contain new-line delimited lists of AOIs and the json files contain messages associated with different errors observed within AOIs in the batch. _Each AOI corresponds to a pipeline within the batch_. The most important top line files are:

* `unique_success_aoi_names.txt`: This contains the list of successful AOIs/successful pipelines within this batch run
* `unique_fail_aoi_names.txt`: This contains the list of failed AOIs/failed pipelines within this batch run 

The other files generated by `cloudwatch_reports.py` are:

* `agr_mos_errors_aois.txt`: This is the list of AOIs that failed due to errors logged by the agreement or mosaic jobs in a pipeline
* `agr_mos_erros_results.json`: This is a json file that contains the specific error messages associated with an agreement mosaic job failure for the AOIs that failed at these stages
* `early_exit_aois.txt`: This is a list of AOIs that exited early without producing data without a failure of the pipeline code itself. This can be caused for a number of reasons. For example, there could be no HAND data in the AOI being queried or a container could have crashed.
* `early_exit_aois.json`: This is a json file that contains the full CloudWatch logstream names for the AOIs that exited early. These are provided to allow the user to quickly look up the full pipeline logs for the AOI that exited early.
* `failed_results.json`: This is a json file that records messages from the pipeline job itself that are associated with pipeline failure. AOIs obtained from this list are combined with AOIs that failed at the inundate, mosaic, or agreement stages to get a list of all failed AOIs
* `ignorable_errors_aois.txt`: This list contains AOIs that give errors that don't necessarily indicate pipeline failure. For example, an agreement_maker job could log an error that no metrics could be obtained from an agreement map that contains no masked data. These are considered successful AOIs.
* `ignorable_errors_results.json`: This file records the specific error messages that are considered ignorable. This allows the user to evaluate if errors are being considered ignorable that are currently actually of concern.
* `inundate_errors_aois.txt`: This list contains AOIs that failed because one of the inundate jobs failed in way that indicates a non-successful pipeline run.
* `inundate_errors_results.json`: This file records the specific error messages that caused the failure of the inundate job/s.
* `job_errors_aois.txt`: This list records jobs that are recorded as failed by Nomad itself during a pipeline run. Nomad can error out a job because of an error caused by the Nomad scheduler or by the job itself.
* `job_errors_results.json`: This file contains the full pipeline job CloudWatch logstream names for the AOIs for which Nomad reported job errors. This makes it possible to fully inspect the full logs to further assess why a job is failing
* `silent_failures.txt`: This is a list of AOIs for which there is no explicit failure or success message in the pipeline job. This is considered a failure. The most common reason for this type of failure is stopping the batch early and purging the batches jobs so a new batch can be run.
`sorted-run-list.txt`: This is a sorted list of the AOI names in the batch of pipelines being analyzed. This is the list of AOI names that the code uses to perform set operations to arrive at the lists of success, failures, etc.
`success_results.json`: This list contains the full CloudWatch logstream names for the successful AOIs for easy lookup of the pipeline logs in CloudWatch.

By inspecting these files it is possible to get an overview of why pipelines are failing. If the failures are for reasons having to do with the Nomad deployment or external APIs then resubmitting the AOIs listed in `unique_fail_aoi_names.txt` is often enough to reprocess the pipelines. If the warning or error messages pointed to by the various report json files and CloudWatch logs indicate a code failure then it is necessary to modify one or more of: the job definitions in `job_defs/test/`, the code in the `src` directory, or in one of `autoeval-jobs` files.

# Interpreting local reports

When pipelines are run inside the Parallel Works environment they persist there logs to the local drive. This means that a different script (`local_reports.py`) is necessary to process these logs since the CloudWatch query language can't be used to analyze them. The format for the local reports is similar to that of the CloudWatch reports with a few differences. The top line files are still:

* `unique_success_aoi_names.txt`
* `unique_fail_aoi_names.txt`

Most of the other aoi list file names are the same as well:
* `agr_mos_errors_aois.txt`
* `early_exit_aois.txt`
* `ignorable_errors_aois.txt`
* `inundate_errors_aois.txt`
* `job_errors_aois.txt`
* `silent_failures.txt`
* `sorted-run-list.txt`

All the above file names signify the same thing they do for the Cloudwatch logs.

There are two additional filenames that are used to create the other final lists. These can be disregarded:
* `failed_aois.txt`
* `success_aois.txt`

The local reports don't contain as many json files since the json files that only point to cloudwatch logs aren't created. There are four possible json  files that be included in a local report directory:

* job_errors.json: Errors within the pipeline job itself that cause a pipeline to fail
* agr_mos_errors.json: Errors that cause AOIs to fail that are associated with the agreement or mosaic jobs
* inundate_errors.json: Errors that cause AOIs to fail that are associated with the inundate jobs
* ignorable_errors.json: Job errors that are considered ignorable

These files are only created if there are error or warning messages to write to them.

The format for the local report json files is a list of json objects where each object contains the following fields:

```
[
  {
    "aoi": "aoi_name_here",
    "job": "job_type_here",
    "task": "task_name_here", 
    "file": "relative/path/to/stderr/file",
    "message": "The actual error message text"
  },
  ...
]
```


